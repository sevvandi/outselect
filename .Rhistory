}else if (s==2){
col_nums <- 1:dim(features_all)[2]
ftr_subset <- features_all
}
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
# Create n equally size folds
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
cat("Starting", n, "fold cross validation. This will take some time... \n")
i
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
dim(trainLabels)
dim(trainData)
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
result_table[1,]
mean(result_table[1,])
default_accuracy <- apply(perfs, 2, table)*100/dim(perfs)[1]
default_accuracy
head(ftr_subset)
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
new_order
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
folds
testSources
file_source
testSources
testIndices <- which(file_source %in% testSources)
unique(filenames(testIndices))
unique(filenames[testIndices])
unique(file_source[testIndices])
testSources
length(file_source)
which(folds==i,arr.ind=TRUE)
testIndices
i <- 2
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
testSources
testIndices
n
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,n=10)
res_d2p1$def_acc
res_d2p1$results
res_d2p1$mean_acc
res_d2p1$def_acc
folds
testSources
file_source
devtools::load_all()
# absolute performance  1 or 0
data(abs_perfs_all)
colnames(abs_perfs_all)
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
usethis::use_package("randomForest")
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
library("randomForest")
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
devtools::load_all(0)
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
res_d2p1$def_acc
res_d2p1$results
res_d2p1$mean_acc
res_d1p1 <- CrossValidateModels(d=1,p=1,s=1, n=10)
res_d1p1$mean_acc
res_d1p1$def_acc
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
devtools::load_all()
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
rm_cols
col_nums <- setdiff(col_nums,rm_cols)
col_nums
ftr_subset <- features_mm[ ,col_nums]
ftr_subset <- apply(ftr_subset,2,unitize_2)
head(ftr_subset)
data(features_all)
# ---- ALL NORMALIZATION METHODS - PERFORMANCE AND FEATURES
data(features_all)
colnames(features_all)
devtools::load_all()
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
col_nums <- 1:dim(features_mm)[2]
ftr_subset1 <- features_mm
sds <- apply(ftr_subset1, 2, sd)
rm_cols <- c(1, which(sds==0)) # to remove the filename
col_nums <- setdiff(col_nums,rm_cols)
ftr_subset <- features_mm[ ,col_nums]
sds
features_mm$OPO_GComp_Out_SD_1
apply(features_all,2,function(x) (sum(is.na(x))))
which( apply(features_all,2,function(x) (sum(is.na(x)))) >0 )
nas <- apply(features_all,2, function(x) sum(is.na(x)) )
which(nas >0 )
usethis::use_data(features_all)
devtools::load_all(0)
devtools::load_all()
data(features_all)
data(features_mm)
nas <- apply(features_mm,2, function(x) sum(is.na(x)) )
which(nas >0 )
cols_na <- which(nas >0 )
features_mm[is.na]
i <- 1
ftr <- features_mm[ ,cols_na[i]]
ftr
which(is.na(ftr) )
ftr[which(is.na(ftr) ) ]
ftr[which(is.na(ftr) ) ] <- 0
ftr[which(is.na(ftr) ) ]
ftr
for(i in 1:length(cols_na)){
ftr <- features_mm[ ,cols_na[i]]
ftr[which(is.na(ftr) ) ] <- 0
features_mm[ ,cols_na[i]] <- ftr
}
perf_vals_mm <- read.csv(paste(folder, "Performance_Min_Max.csv", sep=""))
nas <- apply(features_mm,2, function(x) sum(is.na(x)) )
cols_na <- which(nas >0 )
cols_na
usethis::use_data(features_mm, overwrite = TRUE)
col_nums <- 1:dim(features_mm)[2]
ftr_subset1 <- features_mm
sds <- apply(ftr_subset1, 2, sd)
sds
rm_cols <- c(1, which(sds==0)) # to remove the filename
rm_cols
col_nums <- setdiff(col_nums,rm_cols)
ftr_subset <- features_mm[ ,col_nums]
devtools::load_all()
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
rm_cols
col_nums
col_nums <- setdiff(col_nums,rm_cols)
col_nums
ftr_subset <- features_mm[ ,col_nums]
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
uniq_f_s
filenames
length(filenames)
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
uniq_f_s
file_source
length(filenames)
devtools::load_all()
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2, n=10)
res_d1p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
devtools::load_all()
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
infs <- apply(features_mm, 2, function(x) sum(is.infinite(x)))
which(infs>0)
indx <- apply(features_mm, 2, function(x) any(is.na(x) | is.infinite(x)))
colnames[indx]
colnames(features_mm)[indx]
apply(features_mm,2,function(x)sum(is.infinite(x)) )
infs <- apply(features_mm,2,function(x)sum(is.infinite(x)) )
which(infs > 0)
infs <- apply(features_mm,2,function(x)sum(is.nan(x)) )
which(infs > 0)
infs <- apply(features_mm,2,function(x)sum(is.infinite(x)) )
data(perf_vals_mm)
infs <- apply(perf_vals_mm,2,function(x)sum(is.infinite(x)) )
which(infs > 0)
devtools::load_all()
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=1,p=1,s=1,n=10)
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2,n=10)
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
rm_cols
col_nums <- setdiff(col_nums,rm_cols)
ftr_subset <- features_all[ ,-rm_cols]
2
rm_cols
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
p
data(abs_perfs_all)
perfs <- abs_perfs_all
# features are in ftr_subset
# performance values in perfs
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
file_source
filenames
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
file_source
ftr_subset
ftr_subset <- apply(ftr_subset,2,unitize_2)
colnames(ftr_subset)
rm_cols
ftr_subset <- apply(ftr_subset,2,unitize_2)
devtools::load_all()
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
rm_cols
ftr_subset[,29]
sum(is.nan(ftr_subset[,29]))
sum(is.infinite(ftr_subset[,29]))
which(is.infinite(ftr_subset[,29]))
ftr_subset[10932, 29]
infs <- apply(features_mm,2,function(x)sum(is.infinite(x)) )
which(infs > 0)
infs <- apply(features_mm,1,function(x)sum(is.infinite(x)) )
which(infs > 0)
infcols <- function(x)
{
for (i in 1:ncol(x)){
if ( sum( is.infinite(x[,i]) ) ) > 0
out <- c(out, i)
}
return(out)
}
infcols <- function(x){
out <- c()
for (i in 1:ncol(x)){
if ( sum( is.infinite(x[,i]) ) ) > 0
out <- c(out, i)
}
return(out)
}
infcols <- function(x){
out <- c()
for(i in 1:ncol(x)){
if ( sum( is.infinite(x[,i]) ) ) > 0
out <- c(out, i)
}
return(out)
}
infcols <- function(x){
out <- c()
for(i in 1:ncol(x)){
if (sum(is.infinite(x[,i])) > 0 )
out <- c(out, i)
}
return(out)
}
infc <- infcols(features_mm)
infc
max(features_mm[,30])
max(features_mm[,31])
features_mm <- RemoveInfiniteValues(features_mm, infc)
apply(features_mm,2,max)
apply(features_mm[, -1],2,max)
apply(features_mm[, -1],2,min)
min(apply(features_mm[, -1],2,min))
max(apply(features_mm[, -1],2,max))
usethis::use_data(features_mm, overwrite = TRUE)
f2 <- apply(features_mm[, -1],2,unitize_2)
data(features_mm)
f2 <- apply(features_mm[, -1],2,unitize_2)
features_mm[,30]
data(features_mm)
max(apply(features_mm[, -1],2,max))
min(apply(features_mm[, -1],2,min))
f2 <- apply(features_mm[, -1],2,unitize_1)
apply(f2, 2, max)
apply(f2, 2, min)
devtools::load_all()
res_d1p1s2 <- CrossValidateModels(d=1,p=1,s=2,n=10)
res_d1p1s2$mean_acc
res_d1p1s2$def_acc
getwd()
write_folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/Conf_Paper/"
write.csv(res_d1p1s2$def_acc, paste(write_folder, "Default_Accuracy_All_Ftrs_Min_Max.csv", sep=""), row.names = FALSE)
write.csv(res_d1p1s2$def_acc, paste(write_folder, "10_fold_CV_Accuracy_All_Ftrs_Min_Max.csv", sep=""), row.names = FALSE)
write.csv(res_d1p1s2$results, paste(write_folder, "10_fold_CV_Accuracy_All_Ftrs_Min_Max.csv", sep=""), row.names = FALSE)
res_d1p1s2$mean_acc
res_d1p1s2$mean_acc*100
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
devtools::load_all()
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
# ---- ALL NORMALIZATION METHODS - PERFORMANCE AND FEATURES
data(features_all)
apply(features_all, 2, max)
which(is.infinite(apply(features_all, 2, max)))
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
rm_cols
rm_cols <- c(1, which(sds==0))
rm_cols
col_nums <- setdiff(col_nums,rm_cols)
col_nums
ftr_subset <- features_all[ ,-rm_cols]
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
}
# features are in ftr_subset
# performance values in perfs
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
devtools::load_all()
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
nas <- apply(features_all,2, function(x) sum(is.na(x)) )
which(nas >0 )
which(nas >0 )
cols_na
nas <- apply(features_all,2, function(x) sum(is.na(x)) )
which(nas >0 )
devtools::load_all()
fname <- filenames[ll]
res_d2p1s2 <- CrossValidateModels(d=2,p=1,s=2, n=10)
devtools::load_all()
?TrainModels
?CrossValidateModels
devtools::document
devtools::document()
?CrossValidateModels
devtools::document()
devtools::document()
?CrossValidateModels
devtools::document()
?CrossValidateModels
library("RWeka")
dat <- read.csv("C:/Users/Sevvandi/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/DataFolder_Input/Datasets_To_Test_Functions/abalone_C1_P02_V07.arff")
dat <- read.arff("C:/Users/Sevvandi/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/DataFolder_Input/Datasets_To_Test_Functions/abalone_C1_P02_V07.arff")
ftrs <- ComputeMetaFeaturesMM(dat)
usethis::use_package("infotheo")
install.packages("infothoe")
install.packages("infothoe")
install.packages("infotheo")
usethis::use_package("infotheo")
dat <- read.arff("C:/Users/Sevvandi/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/DataFolder_Input/Datasets_To_Test_Functions/abalone_C1_P02_V07.arff")
ftrs <- ComputeMetaFeaturesMM(dat)
??Discretize
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
?mutinformation
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
install.packages("moments")
usethis::use_package("moments")
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
?FNN
?fnn
??FNN\
??FNN
install.packages("FNN")
?kde
??kde
usethis::use_package("ks")
install.packages("ks")
usethis::use_package("ks")
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
devtools::load_all()
usethis::use_package("igraph")
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
devtools::load_all()
ftrs <- ComputeMetaFeaturesMM(dat)
system.time(ftrs <- ComputeMetaFeaturesMM(dat))
devtools::load_all()
fit <- TrainModels(1,1,1)
fit
devtools::load_all()
fit <- TrainModels(1,1,1)
fit <- TrainModels(1,1,1)
devtools::load_all()
fit <- TrainModels(1,1,1)
preds <- PredictPerformance(ftrs, fit)
?randomForest::predict
?randomForest
predit.randomForst
if( dim(ftrs)[2] > length(models$cols) ){
x <- ftrs[,models$cols]
}
models <- fit
if( dim(ftrs)[2] > length(models$cols) ){
x <- ftrs[,models$cols]
}
x
pred$cof <- predict(models$cof,newdata=x, type="prob")
pred$cof <- predict(models$cof,newdata=x)
pred$cof <- predict(models$cof,newdata=x)
models$cof
pred$cof <- predict(models$cof,newdata=as.matrix(x))
devtools::load_all()
