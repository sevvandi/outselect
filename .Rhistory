col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_3', 'OPO_Den_Out_SD_3', 'OPO_Res_Out_95P_3', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_3')
which(grepl(col_list, colnames(features)))
grepl(col_list, colnames(features))
which(col_list %in% colnames(features) )
which(colnames(features) %in% col_list )
col_nums <- which(colnames(features) %in% col_list )
ftr_subset <- features[,col_nums]
apply(ftr_subset,2,is.na)
dim(ftr_subset)
apply(ftr_subset,2,function(x) sum(is.na(x)) )
apply(ftr_subset,2,function(x) sum(is.infinite(x)) )
apply(ftr_subset,2,function(x) sum(is.nan(x)) )
library(randomForest)
library(RandomForest)
library(randomForest)
library("randomForest")
install.packages("randomForest")
library(randomForest)
colnames(abs_perfs)
?randomForest
mod_cof <- randomForest::randomForest(ftr_subset, as.factor(abs_perfs[,1]) )
?predict
load(features)
data(features)
data(abs_perfs)
col_nums <- which(colnames(features) %in% col_list )
ftr_subset <- features[,col_nums]
# Train models
mod_cof <- randomForest::randomForest(ftr_subset, as.factor(abs_perfs[,1]) )
mod_fabod <- randomForest::randomForest(ftr_subset, as.factor(abs_perfs[,2]) )
devtools::load_all()
models_abs <- TrainModels(1)
models_abs$cof
models_abs$fabod
TrainModels(5)
devtools::load_all()
TrainModels(5)
25
25+77+35+41
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/To_Github/Data/Features_and_Performance/"
features_all <- read.csv(paste(folder, "Features.csv", sep=""))
usethis::use_data(features_all)
perf_vals_all <- read.csv(paste(folder, "Performance_values.csv", sep=""))
usethis::use_data(perf_vals_all)
col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_3', 'OPO_Den_Out_SD_3', 'OPO_Res_Out_95P_3', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_3')
# ---- Getting absolute good performance
abs_perfs_all <- apply(perf_vals_all, 2, function(x) ifelse(x>=0.8,1,0))
usethis::use_data(abs_perfs_all)
# ---- Getting relative good performance 5%
rel_perfs_0.05_all <- EpsilonGood(perf_vals_all, 0.05)
usethis::use_data(rel_perfs_0.05_all)
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/Sent_To_Andres/Conf_Paper/Min_Max/"
features_mm <- read.csv(paste(folder, "Features.csv", sep=""))
colnames(features_mm)
features_mm <- features_mm[ ,-1]
head(features_mm)
usethis::use_data(features_mm)
perf_vals_mm <- read.csv(paste(folder, "Performance_Min_Max.csv", sep=""))
usethis::use_data(perf_vals_all)
usethis::use_data(perf_vals_mm)
perf_vals_all <- read.csv(paste(folder, "Performance_values.csv", sep=""))
# ---- All normalization methods
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/To_Github/Data/Features_and_Performance/"
perf_vals_all <- read.csv(paste(folder, "Performance_values.csv", sep=""))
usethis::use_data(perf_vals_all)
head(perf_vals_mm)
perf_vals_mm <- perf_vals_mm[ ,  -1]
usethis::use_data(perf_vals_mm, overwrite=TRUE)
abs_perfs_mm <- apply(perf_vals_mm, 2, function(x) ifelse(x>=0.8,1,0))
head(abs_perfs_mm)
tail(abs_perfs_mm)
abs_perfs_mm <- apply(perf_vals_mm, 2, function(x) ifelse(x>=0.8,1,0))
usethis::use_data(abs_perfs_mm)
rel_perfs_0.05_mm <- EpsilonGood(perf_vals_mm, 0.05)
usethis::use_data(rel_perfs_0.05_mm)
# Only Min_Max normalization method
data(features_mm)
col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_1', 'OPO_Den_Out_SD_1', 'OPO_Res_Out_95P_1', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_1')
col_nums <- which(colnames(features_all) %in% col_list )
col_nums <- which(colnames(features_all) %in% col_list )
ftr_subset <- features_mm[ ,col_nums]
col_nums
col_nums <- which(colnames(features_mm) %in% col_list )
col_nums
ftr_subset <- features_mm[ ,col_nums]
devtools::load_all()
table(perfs)
data(abs_perfs_all)
perfs <- abs_perfs_all
table(perfs)
apply(perfs, 2, table)
apply(perfs, 2, table)*100/dim(perfs)[1]
apply(perfs, 2, table)*100/dim(perfs)[1][1,]
default_accuracy <- apply(perfs, 2, table)*100/dim(perfs)[1]
default_accuracy
default_accuracy[1,]
default_accuracy <- default_accuracy[1, ]
devtools::load_all
devtools::load_all(0)
devtools::load_all()
## Test methods
res <- CrossValidateModels(d=1,p=1)
res
## Test methods
res <- CrossValidateModels(d=1,p=1, s=1, n=5)
res
## Test methods
res <- CrossValidateModels(1,1)
res
## Test methods
res <- CrossValidateModels(1,1)
d
(d!=1)
(d!=2)
devtools::load_all()
## Test methods
res <- CrossValidateModels(d=1,p=1)
devtools::load_all()
## Test methods
res <- CrossValidateModels(d=1,p=1)
devtools::load_all()
res <- CrossValidateModels(d=1,p=1)
devtools::load_all()
devtools::load_all()
res <- CrossValidateModels(d=1,p=1)
devtools::load_all()
## Test methods
res <- CrossValidateModels(d=1,p=1)
res$def_acc
res$mean_acc
res$results
res_d2p1 <- CrossValidateModels(d=2,p=1)
res_d2p1$def_acc
res_d2p1$results
res_d2p1$mean_acc
res_d2p1$def_acc
data(abs_perfs_all)
dim(abs_perfs_all)
head(abs_perfs_all)
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,n=10)
# ---- All normalization methods
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/To_Github/Data/Features_and_Performance/"
filenames_all <- read.csv(paste(folder, "Filenames.csv", sep=""))
colnames(features_all)
# ---- Only min-max method
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/Sent_To_Andres/Conf_Paper/Min_Max/"
features_mm <- read.csv(paste(folder, "Features.csv", sep=""))
# ---- Only min-max method
folder <- "//ad.monash.edu/home/User098/skandan/Documents/Research/Preprocessing_Anomalies_And_Instance_Spaces/Sent_To_Andres/Conf_Paper/Min_Max/"
features_mm <- read.csv(paste(folder, "Features.csv", sep=""))
usethis::use_data(features_mm, overwrite = TRUE)
data("features_all")
colnames(features_all)
data(features_mm)
colnames(features_mm)
features_all$filename
filenames <- features_all$filename
length(filenames)
file.source <-c()
for(i in 1:length(filenames)){
fname <- filenames[i]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
wierd.list <- c(wierd.list, i)
regobj <- regobj1
}
end.ind <- regobj[1]-1
file.source <- c(file.source, substring(fname, 1, end.ind))
}
file.source
wierd.list
file_source <-c()
for(i in 1:length(filenames)){
fname <- filenames[i]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
file_source
unique(file_source)
as.numeric(unique(file_source))
unique(file_source)
1:length(unique(file_source))
uniq_f_s <- unique(file_source)
1:length(uniq_f_s)
num_ufs <- 1:length(uniq_f_s)
sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
n <- 10
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
folds
new_order
new_order <- sample(uniq_f_s,length(uniq_f_s))
i <- 1
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testSources
testIndices <- which(uniq_f_s %in% testSources)
testIndices
testIndices <- which(file_source %in% testSources)
testIndices
length(testIndices)
cat("Starting ", n, " fold cross validation. This will take some time ... \n")
cat("Starting", n, "fold cross validation. This will take some time ... \n")
cat("Starting", n, "fold cross validation. This will take some time... \n")
file_source <-c()
for(i in 1:length(filenames)){
fname <- filenames[i]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
# Create n equally size folds
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
cat("Starting", n, "fold cross validation. This will take some time... \n")
i <- 1
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
dim(trainData)
length(trainLabels)
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
dim(testData)
dim(trainData)
dim(testLabels)
dim(trainLabels)
dim(perfs)
dim(features_all)
dim(features_mm)
dim(perf_vals_mm)
dim(perf_vals_all)
dim(perf_vals_mm)
dim(features_mm)
data(features_all)
filenames <- features_all$filename
if(s==1){
col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_3', 'OPO_Den_Out_SD_3', 'OPO_Res_Out_95P_3', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_3')
col_nums <- which(colnames(features_all) %in% col_list )
ftr_subset <- features_all[ ,col_nums]
}else if (s==2){
col_nums <- 1:dim(features_all)[2]
ftr_subset <- features_all
}
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
}
# features are in ftr_subset
# performance values in perfs
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(i in 1:length(filenames)){
fname <- filenames[i]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
# Create n equally size folds
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
cat("Starting", n, "fold cross validation. This will take some time... \n")
i
i <- 1
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
file_source
uniq_f_s <- unique(file_source)
# Create n equally size folds
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
cat("Starting", n, "fold cross validation. This will take some time... \n")
i <- 1
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
dim(testData)
dim(trainData)
dim(testLabels)
dim(trainLabels)
p <- 1
d <- 2
# ---- ALL NORMALIZATION METHODS - PERFORMANCE AND FEATURES
data(features_all)
filenames <- features_all$filename
if(s==1){
col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_3', 'OPO_Den_Out_SD_3', 'OPO_Res_Out_95P_3', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_3')
col_nums <- which(colnames(features_all) %in% col_list )
ftr_subset <- features_all[ ,col_nums]
}else if (s==2){
col_nums <- 1:dim(features_all)[2]
ftr_subset <- features_all
}
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
s <- 1
# ---- ALL NORMALIZATION METHODS - PERFORMANCE AND FEATURES
data(features_all)
filenames <- features_all$filename
if(s==1){
col_list <- c('SNR', 'OPO_Res_KNOut_95P_1', 'OPO_Out_DenOut_1_3', 'OPO_Den_Out_SD_3', 'OPO_Res_Out_95P_3', 'OPO_LocDenOut_Out_95P_1', 'OPO_GDeg_Out_Mean_1', 'OPO_GComp_PO_Q95_3')
col_nums <- which(colnames(features_all) %in% col_list )
ftr_subset <- features_all[ ,col_nums]
}else if (s==2){
col_nums <- 1:dim(features_all)[2]
ftr_subset <- features_all
}
if(p==1){
# absolute performance  1 or 0
data(abs_perfs_all)
perfs <- abs_perfs_all
}else if(p==2){
# relative performance  1 or 0
data(rel_perfs_0.05_all)
perfs <- rel_perfs_0.05_all
}
result_table <- matrix(0, nrow=n, ncol=dim(perfs)[2])
# Cross validation on file source as many variants of the same file exist
file_source <-c()
for(ll in 1:length(filenames)){
fname <- filenames[ll]
regobj1 <- regexpr("_C", fname)
regobj2 <- regexpr("_withoutdupl", fname)
if(regobj1[1]<0){
regobj <- regobj2
}else if(regobj2[1]<0){
regobj <- regobj1
}else{
regobj <- regobj1
}
end.ind <- regobj[1]-1
file_source <- c(file_source, substring(fname, 1, end.ind))
}
uniq_f_s <- unique(file_source)
# Create n equally size folds
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
cat("Starting", n, "fold cross validation. This will take some time... \n")
i
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
dim(trainLabels)
dim(trainData)
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
result_table[1,]
mean(result_table[1,])
default_accuracy <- apply(perfs, 2, table)*100/dim(perfs)[1]
default_accuracy
head(ftr_subset)
set.seed(1234)
new_order <- sample(uniq_f_s,length(uniq_f_s))
new_order
folds <- cut(seq(1,length(uniq_f_s)),breaks=n,labels=FALSE)
folds
testSources
file_source
testSources
testIndices <- which(file_source %in% testSources)
unique(filenames(testIndices))
unique(filenames[testIndices])
unique(file_source[testIndices])
testSources
length(file_source)
which(folds==i,arr.ind=TRUE)
testIndices
i <- 2
testSources <- new_order[which(folds==i,arr.ind=TRUE)]
testIndices <- which(file_source %in% testSources)
# Segement your data by fold
testData <- ftr_subset[testIndices, ]
trainData <- ftr_subset[-testIndices, ]
testLabels <- perfs[testIndices, ]
trainLabels <- perfs[-testIndices, ]
for(j in 1:dim(perfs)[2]){
cat("Fold ", i, " Method " , j, "... \n")
model <- randomForest(trainData, as.factor(trainLabels[ ,j]))
preds <- predict(model, testData, type="class")
result_table[i,j] <- sum(preds==testLabels)/length(testLabels)
print(paste("Accuracy",  result_table[i,j]) )
}
testSources
testIndices
n
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,n=10)
res_d2p1$def_acc
res_d2p1$results
res_d2p1$mean_acc
res_d2p1$def_acc
folds
testSources
file_source
devtools::load_all()
# absolute performance  1 or 0
data(abs_perfs_all)
colnames(abs_perfs_all)
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
usethis::use_package("randomForest")
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
library("randomForest")
devtools::load_all()
res_d2p1 <- CrossValidateModels(d=2,p=1,s=1,n=10)
